<!DOCTYPE HTML>
<!--
    GenSign Workshop @ CVPR 2026 - Generative AI for Sign Language
    Based on the "Stellar" template by HTML5 UP (CCA 3.0 licence)
-->
<html>
    <head>
        <title>GenSign - CVPR 2026 Workshop</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <meta property="og:title" content="GenSign - CVPR 2026 Workshop" />
        <meta property="og:description" content="Generative AI for Sign Language - CVPR 2026 Workshop" />
        <link rel="stylesheet" href="assets/css/main.css" />
        <link rel="stylesheet" href="assets/css/gensign.css" />
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
        <!-- Font Awesome for icons -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    </head>
    <body class="">

        <!-- Wrapper -->
            <div id="wrapper">
                <!-- Header -->
                    <header id="header" class="alt">
                        <div class="hero-bg"></div>        
                        <h1><span style="font-size: 1.5em; font-weight: 600; font-family: 'Montserrat', sans-serif;">GenSign</span><br> 
                        <div style="height:10px;"></div> 
                        Generative AI for Sign Language</h1>
                        <div style="height:16px;"></div>
                        <span style="font-size: 1.1em;"><p><strong>CVPR 2026 Workshop</strong></p></span>
                        <span style="font-size: 1.1em;"><p>June 2026 --- Denver Colorado</p></span>
                    </header>
                <!-- Nav -->
                    <nav id="nav">
                        <button id="nav-toggle" class="nav-toggle" aria-label="Toggle navigation">
                            <span></span>
                            <span></span>
                            <span></span>
                        </button>
                        <ul id="nav-menu">
                            <li><a href="#intro" class="active">Overview</a></li>
                            <li><a href="#callforpapers">Call for Papers</a></li>
                            <li><a href="#schedule">Schedule</a></li>
                            <li><a href="#speakers">Speakers</a></li>
                            <li><a href="#organizers">Organizers</a></li>
                        </ul>
                    </nav>

                <!-- Main -->
                    <div id="main">

                        <!-- Introduction -->
                        <section id="intro" class="main" style="padding-top: 0.5em;">
                                <div class="spotlight">
                                    <div class="content">
                                        <header class="major">
                                            <h2>Overview</h2>
                                        </header>
                                        <p class="overview-note"><em>Note: ASL interpreters will be available throughout the workshop.</em></p>
                                    <ul>
                                        <p>Welcome to <strong><span>GenSign</span></strong>: The 1st Workshop on Generative AI for Sign Language at CVPR 2026!</strong></p>
                                        <div style="height: 0.5em;"></div>
                                        
                                        <p>Recent advances in sequential generative models and diffusion-based approaches offer a promising pathway to produce high-quality translations, synthesize realistic and expressive digital signers, expand low-resource sign datasets, and foster more inclusive communication between deaf and hearing communities.</p>
                                        <div style="height: 0.5em;"></div>
                                        
                                        <p>This workshop brings together researchers from computer vision, natural language processing, linguistics, and accessibility studies to explore the frontiers of generative modeling for sign language and to foster responsible, human-centered AI systems that understand and communicate through this uniquely visual language. Specifically, we aim to:</p>
                                        
                                        <ul class="workshop-objectives">
                                            <li><strong><span>Advance generative approaches for sign language understanding and generation</span></strong>, including high-quality translations, realistic and expressive digital signer synthesis, and expansion of low-resource sign datasets.</li>
                                            <li><strong><span>Leverage modern sequential and diffusion-based models</span></strong> to address realism, controllability, and data scarcity in low-resource sign language settings.</li>
                                            <li><strong><span>Promote responsible and inclusive AI systems</span></strong> that respect linguistic structure and meaningfully support deaf and hard-of-hearing communities.</li>
                                        </ul>
                                    </ul>
                                    </div>
                                </div>
                            </section>

                        <!-- Call for Papers -->
                            <section id="callforpapers" class="main">
                                <header class="major">
                                    <h2>Call for Papers</h2>
                                </header>
                                <p>We welcome contributions covering all aspects of generative AI for sign language:</p>
                                
                                <div class="topic-cards">
                                    <div class="topic-card">
                                        <h3>ü§ü Advances in Sign Language Processing</h3>
                                        <ul>
                                            <li>Sign Language Production and Synthesis</li>
                                            <li>Sign Language Recognition and Translation</li>
                                            <li>Inclusive Technology and Real-world Impact</li>
                                        </ul>
                                    </div>

                                    <div class="topic-card">
                                        <h3>üß† Human-centric Generative Foundation Models</h3>
                                        <ul>
                                            <li>Human-Centric Representation Learning</li>
                                            <li>Human Motion & Gesture Synthesis</li>
                                            <li>Personalization & Social Alignment</li>
                                        </ul>
                                    </div>

                                    <div class="topic-card">
                                        <h3>üìä Evaluation, Datasets and Benchmarks</h3>
                                        <ul>
                                            <li>Semantic and Human-centric Metrics</li>
                                            <li>Large-Scale Benchmarking</li>
                                            <li>Automatic Annotation Tools</li>
                                        </ul>
                                    </div>

                                    <div class="topic-card">
                                        <h3>ü§ù Ethics, Inclusivity & Cultural Competence</h3>
                                        <ul>
                                            <li>Participatory Design in AI</li>
                                            <li>Bias, Privacy and Data Sovereignty</li>
                                            <li>Cultural and Linguistic Authenticity</li>
                                        </ul>
                                    </div>
                                </div>

                                <div class="cfp-tracks">
                                    <div class="cfp-card">
                                        <h3>Submission Track 1: Proceedings Track</h3>
                                        <p>Submissions must present original, unpublished research and follow the CVPR 2026 template.</p>
                                        <ul>
                                            <li><strong>Length</strong>: 5-8 pages (main text), excluding references and optional appendices</li>
                                            <li><strong>Review Process</strong>: Double-blind peer review via OpenReview, all manuscripts must be fully anonymized</li>
                                            <li><strong>Publication</strong>: Accepted papers will be published in workshop proceedings</li>
                                            <li><strong>Code & Data</strong>: Open-sourcing is encouraged but not mandatory</li>
                                            <li><strong>Presentation</strong>: All accepted papers are expected to be presented in person at the workshop</li>
                                        </ul>
                                        <h4>Important Dates (AoE)</h4>
                                        <ul class="key-dates">
                                            <li><strong>Paper Submission Deadline:</strong> March 14, 2026</li>
                                            <li><strong>Author Notification:</strong> March 20, 2026</li>
                                            <li><strong>Camera Ready:</strong> April 10, 2026</li>
                                        </ul>
                                    </div>

                                    <div class="cfp-card">
                                        <h3>Submission Track 2: Non-Proceedings Track</h3>
                                        <p>A flexible, non-archival track for sharing work without restrictive formatting or page limits.</p>
                                        <ul>
                                            <li>Works-in-progress and preliminary results</li>
                                            <li>Open datasets, technical reports, and recent submissions</li>
                                            <li>Position papers and conceptual frameworks</li>
                                            <li>Previously published work accepted</li>
                                        </ul>
                                        <h4>Important Dates (AoE)</h4>
                                        <ul class="key-dates">
                                            <li><strong>Paper Submission Deadline:</strong> April 4, 2026</li>
                                            <li><strong>Author Notification:</strong> April 14, 2026</li>
                                            <li><strong>Camera Ready:</strong> May 11, 2026</li>
                                        </ul>
                                    </div>
                                </div>
                                <ul class="actions" style="margin-top: 1.5em; justify-content: center;">
                                    <li>
                                        <a href="#" class="button primary">
                                            <span class="button-line">Submit Paper</span>
                                            <span class="button-line line-secondary">(OpenReview - Coming Soon)</span>
                                        </a>
                                    </li>
                                    <li><a href="https://github.com/cvpr-org/author-kit/archive/refs/tags/CVPR2026-v1(latex).zip" class="button">Download Template</a></li>
                                </ul>
                            </section>

                        <!-- Speakers -->
                        <section id="speakers" class="main special">
                        <header class="major">
                            <h2>Invited Speakers</h2>
                        </header>
                        <p style="margin-bottom: 2em;">We are honored to host distinguished academic and industry experts on generative AI for sign language. </p>
                        
                        <div class="speakers-list" style="display: flex; flex-direction: column; gap: 2em; margin: 2em 0;">
                            <div class="speaker-card" style="display: flex; gap: 2em; padding: 1.5em; background: #f8f9fa; border-radius: 8px; align-items: center;">
                                <img class="speaker-avatar" src="assets/images/Richard_Bowden.webp" alt="Richard Bowden" style="width: 150px; height: 150px; border-radius: 8px; object-fit: cover; flex-shrink: 0; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.12);">
                                <div style="flex: 1; text-align: left;">
                                    <h3 style="margin: 0 0 0.5em 0; color: #2d3436; text-align: left;"><a href="https://personalpages.surrey.ac.uk/r.bowden/" target="_blank" rel="noopener">Richard Bowden</a></h3>
                                    <p style="color: #636e72; margin: 0.3em 0; font-weight: 600; text-align: left;">University of Surrey & Signapse AI</p>
                                    <p style="color: #636e72; margin: 0.8em 0 0; line-height: 1.6; font-size: 0.95em; text-align: left;">Richard Bowden is Professor of Computer Vision and Machine Learning at the University of Surrey, UK, where he leads the Cognitive Vision Group within the Centre for Vision Speech and Signal Processing. He is also Co-founder and Chief Scientist of Signapse AI. His research focuses on computer vision for human detection, tracking, and understanding. He is a Fellow of the Higher Education Academy, a senior member of the IEEE, a Fellow of the International Association of Pattern Recognition, and Distinguished Fellow of the BMVA.</p>
                                </div>
                            </div>

                            <div class="speaker-card" style="display: flex; gap: 2em; padding: 1.5em; background: #f8f9fa; border-radius: 8px; align-items: center;">
                                <img class="speaker-avatar" src="assets/images/Karen_Livescu.jpg" alt="Karen Livescu" style="width: 150px; height: 150px; border-radius: 8px; object-fit: cover; flex-shrink: 0; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.12);">
                                <div style="flex: 1; text-align: left;">
                                    <h3 style="margin: 0 0 0.5em 0; color: #2d3436; text-align: left;"><a href="https://home.ttic.edu/~klivescu/" target="_blank" rel="noopener">Karen Livescu</a></h3>
                                    <p style="color: #636e72; margin: 0.3em 0; font-weight: 600; text-align: left;">Toyota Technological Institute at Chicago</p>
                                    <p style="color: #636e72; margin: 0.8em 0 0; line-height: 1.6; font-size: 0.95em; text-align: left;">Karen Livescu is Professor at TTIC and a part-time Associate Professor at the University of Chicago. Her research interests span speech and language processing, multimodal learning, and representation learning, with a focus on self-supervised models, speech-text integration, and sign language understanding. She is a Fellow of IEEE and ISCA, and served as a Distinguished Lecturer of the IEEE Signal Processing Society. Her research has been recognized with the Best Paper Award at EMNLP 2024 and ICML Test of Time Runner-Up.</p>
                                </div>
                            </div>

                            <div class="speaker-card" style="display: flex; gap: 2em; padding: 1.5em; background: #f8f9fa; border-radius: 8px; align-items: center;">
                                <img class="speaker-avatar" src="assets/images/Chris_Dyer.jpg" alt="Chris Dyer" style="width: 150px; height: 150px; border-radius: 8px; object-fit: cover; flex-shrink: 0; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.12);">
                                <div style="flex: 1; text-align: left;">
                                    <h3 style="margin: 0 0 0.5em 0; color: #2d3436; text-align: left;"><a href="https://scholar.google.com/citations?user=W2DsnAkAAAAJ&hl=en" target="_blank" rel="noopener">Chris Dyer</a></h3>
                                    <p style="color: #636e72; margin: 0.3em 0; font-weight: 600; text-align: left;">Google DeepMind</p>
                                    <p style="color: #636e72; margin: 0.8em 0 0; line-height: 1.6; font-size: 0.95em; text-align: left;"> Chris Dyer is Principal Scientist at Google DeepMind, where he leads the SignGemma project. His research lies at the intersection of natural language processing, computational linguistics, and machine learning, with significant contributions to statistical and neural machine translation, probabilistic modeling, and large-scale text processing. He co-developed widely used NLP frameworks such as cdec. He has served on program committees for major NLP conferences, including ACL, EMNLP, and NAACL.</p>
                                </div>
                            </div>

                            <div class="speaker-card" style="display: flex; gap: 2em; padding: 1.5em; background: #f8f9fa; border-radius: 8px; align-items: center;">
                                <img class="speaker-avatar" src="assets/images/Abraham_Glasser.jfif" alt="Abraham Glasser" style="width: 150px; height: 150px; border-radius: 8px; object-fit: cover; flex-shrink: 0; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.12);">
                                <div style="flex: 1; text-align: left;">
                                    <h3 style="margin: 0 0 0.5em 0; color: #2d3436; text-align: left;"><a href="https://abrahamglasser.com/" target="_blank" rel="noopener">Abraham Glasser</a></h3>
                                    <p style="color: #636e72; margin: 0.3em 0; font-weight: 600; text-align: left;">Gallaudet University</p>
                                    <p style="color: #636e72; margin: 0.8em 0 0; line-height: 1.6; font-size: 0.95em; text-align: left;">Abraham Glasser is Assistant Professor at Gallaudet University. His research focuses on accessible computing, human‚Äìcomputer interaction, and immersive technologies for Deaf and Hard-of-Hearing users. He has served as the Accessibility Chair of ACM ASSETS, Associate Chair of ACM CHI, etc. He is also a member of the Coalition for Sign Language Equity in Technology (CoSET). He has presented award-winning research at major international venues, including Best Poster at ACM VRST and Impact Paper Award at ACM CSCW.</p>
                                </div>
                            </div>
                        </div>
                        
                        </section>

                        <!-- Schedule -->
                            <section id="schedule" class="main special">
                                <header class="major">
                                    <h2>Workshop Schedule</h2>
                                </header>
                                <p>The workshop will be held as an afternoon session at CVPR 2026 in Nashville, Tennessee.</p>
                                <p style="margin-bottom: 2em;"><em>Note: ASL interpreters will be available throughout the workshop.</em></p>
                                
                                <table class="alt schedule-table">
                                    <thead>
                                        <tr><th>Time</th><th>Session</th><th>Speaker / Details</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>13:00&nbsp;-&nbsp;13:10</td><td>Welcome</td><td>Opening Remarks by Organizers</td></tr>
                                        <tr><td>13:10&nbsp;-&nbsp;13:40</td><td>Keynote&nbsp;1</td><td>TBD</td></tr>
                                        <tr><td>13:40&nbsp;-&nbsp;14:10</td><td>Keynote&nbsp;2</td><td>TBD</td></tr>
                                        <tr><td>14:10&nbsp;-&nbsp;15:00</td><td>Coffee&nbsp;Break</td><td>Poster Session</td></tr>
                                        <tr><td>15:00&nbsp;-&nbsp;15:30</td><td>Keynote&nbsp;3</td><td>TBD</td></tr>
                                        <tr><td>15:30&nbsp;-&nbsp;16:00</td><td>Keynote&nbsp;4</td><td>TBD</td></tr>
                                        <tr><td>16:00&nbsp;-&nbsp;16:30</td><td>Keynote&nbsp;5</td><td>TBD</td></tr>
                                        <tr><td>16:30&nbsp;-&nbsp;17:00</td><td>Oral&nbsp;Presentations</td><td>Selected Paper Presentations</td></tr>
                                        <tr><td>17:00&nbsp;-&nbsp;17:30</td><td>Panel&nbsp;Discussion</td><td>Future Directions & Community Engagement</td></tr>
                                        <tr><td>17:30&nbsp;-&nbsp;17:40</td><td>Closing</td><td>Closing Remarks & Next Steps</td></tr>
                                    </tbody>
                                </table>
                            </section>

                        <!-- Organizers -->
                        <section id="organizers" class="main special">
                        <header class="major">
                            <h2>Organizers</h2>
                        </header>
                        <p style="margin-bottom: 2em;">Our organizing team brings together expertise from computer vision, NLP, linguistics, and accessibility research across multiple continents.</p>

                        <div class="organizers-grid" style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 2em; margin: 2em 0;">
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Hezhen%20Hu.jpg" alt="Hezhen Hu">
                                <h3 style="margin: 0.5em 0;"><a href="https://alexhu.top/" target="_blank" rel="noopener">Hezhen Hu</a></h3>
                                <p style="color: #636e72; margin: 0;">University of Texas at Austin</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Yuecong%20Min.jpg" alt="Yuecong Min">
                                <h3 style="margin: 0.5em 0;"><a href="https://ycmin95.github.io/" target="_blank" rel="noopener">Yuecong Min</a></h3>
                                <p style="color: #636e72; margin: 0;">Institute of Computing Technology, CAS</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Ronglai%20Zuo.png" alt="Ronglai Zuo">
                                <h3 style="margin: 0.5em 0;"><a href="https://2000zrl.github.io/" target="_blank" rel="noopener">Ronglai Zuo</a></h3>
                                <p style="color: #636e72; margin: 0;">Imperial College London</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Oscar%20Koller.png" alt="Oscar Koller">
                                <h3 style="margin: 0.5em 0;"><a href="https://www.microsoft.com/en-us/research/people/oskoller/" target="_blank" rel="noopener">Oscar Koller</a></h3>
                                <p style="color: #636e72; margin: 0;">Microsoft Research, Munich</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/L%C3%A9ore%20Bensabath.png" alt="L√©ore Bensabath">
                                <h3 style="margin: 0.5em 0;"><a href="https://www.linkedin.com/in/leorebensabath/" target="_blank" rel="noopener">L√©ore Bensabath</a></h3>
                                <p style="color: #636e72; margin: 0;">√âcole des Ponts ParisTech</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Wengang%20Zhou.jpg" alt="Wengang Zhou">
                                <h3 style="margin: 0.5em 0;"><a href="https://staff.ustc.edu.cn/~zhwg/" target="_blank" rel="noopener">Wengang Zhou</a></h3>
                                <p style="color: #636e72; margin: 0;">University of Science and Technology of China</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Houqiang%20Li.png" alt="Houqiang Li">
                                <h3 style="margin: 0.5em 0;"><a href="http://staff.ustc.edu.cn/~lihq/en/" target="_blank" rel="noopener">Houqiang Li</a></h3>
                                <p style="color: #636e72; margin: 0;">University of Science and Technology of China</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Stefanos%20Zafeiriou.jpg" alt="Stefanos Zafeiriou">
                                <h3 style="margin: 0.5em 0;"><a href="https://wp.doc.ic.ac.uk/szafeiri/" target="_blank" rel="noopener">Stefanos Zafeiriou</a></h3>
                                <p style="color: #636e72; margin: 0;">Imperial College London & Google</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Xilin%20Chen.png" alt="Xilin Chen">
                                <h3 style="margin: 0.5em 0;"><a href="https://vipl.ict.ac.cn/people/_xlchen/" target="_blank" rel="noopener">Xilin Chen</a></h3>
                                <p style="color: #636e72; margin: 0;">Institute of Computing Technology, CAS</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Hongdong%20Li.png" alt="Hongdong Li">
                                <h3 style="margin: 0.5em 0;"><a href="https://users.cecs.anu.edu.au/~hongdong/" target="_blank" rel="noopener">Hongdong Li</a></h3>
                                <p style="color: #636e72; margin: 0;">Australian National University</p>
                            </div>
                            <div style="padding: 1.5em; background: #f8f9fa; border-radius: 8px; text-align: center;">
                                <img class="organizer-avatar" src="assets/images/Dimitris%20Metaxas.png" alt="Dimitris N. Metaxas">
                                <h3 style="margin: 0.5em 0;"><a href="https://www.cs.rutgers.edu/people/professors/details/dimitris-metaxas" target="_blank" rel="noopener">Dimitris N. Metaxas</a></h3>
                                <p style="color: #636e72; margin: 0;">Rutgers University</p>
                            </div>
                        </div>
                        </section>

                    </div>

                <!-- Footer -->
                    <footer id="footer">
                        <section>
                            <p style="margin-bottom: 0.5em;">Contact: <strong>gensign.workshop@gmail.com</strong> | 
                            <a href="https://cvpr.thecvf.com/">CVPR 2026</a> | 
                            <a href="#">OpenReview</a> | 
                            <a href="https://github.com/cvpr-org/author-kit/archive/refs/tags/CVPR2026-v1(latex).zip">Template</a></p>
                            <ul class="icons" style="margin-top: 0.5em;">
                                <li class="social-link">
                                    <a href="#" class="icon alt" aria-label="Twitter">
                                        <i class="fa-brands fa-twitter" aria-hidden="true"></i>
                                    </a>
                                </li>
                                <li class="social-link">
                                    <a href="#" class="icon alt" aria-label="GitHub">
                                        <i class="fa-brands fa-github" aria-hidden="true"></i>
                                    </a>
                                </li>
                            </ul>
                        </section>
                        <p class="copyright">¬© 2026 GenSign Workshop @ CVPR 2026</p>
                    </footer>

            </div>

        <!-- Scripts -->
            <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
            <script>
                // Smooth scrolling for navigation
                $(document).ready(function() {
                    // Mobile navigation toggle
                    $('#nav-toggle').on('click', function() {
                        $('#nav-menu').toggleClass('nav-open');
                        $(this).toggleClass('active');
                    });
                    
                    // Smooth scroll on click
                    $('a[href^="#"]').on('click', function(e) {
                        var target = $(this.getAttribute('href'));
                        if(target.length) {
                            e.preventDefault();
                            $('html, body').stop().animate({
                                scrollTop: target.offset().top - 60
                            }, 800);
                            
                            // Update active nav
                            $('#nav a').removeClass('active');
                            $(this).addClass('active');
                            
                            // Close mobile menu after clicking a link
                            if ($(window).width() <= 980) {
                                $('#nav-menu').removeClass('nav-open');
                                $('#nav-toggle').removeClass('active');
                            }
                        }
                    });
                    
                    // Update active nav on scroll
                    $(window).scroll(function() {
                        var scrollPos = $(document).scrollTop() + 100;
                        $('#nav a').each(function() {
                            var currLink = $(this);
                            var refElement = $(currLink.attr("href"));
                            if (refElement.length && refElement.position().top <= scrollPos && refElement.position().top + refElement.height() > scrollPos) {
                                $('#nav a').removeClass("active");
                                currLink.addClass("active");
                            }
                        });
                    });
                    
                    // Close mobile menu when clicking outside
                    $(document).on('click', function(e) {
                        if ($(window).width() <= 980) {
                            if (!$(e.target).closest('#nav').length) {
                                $('#nav-menu').removeClass('nav-open');
                                $('#nav-toggle').removeClass('active');
                            }
                        }
                    });
                });
            </script>

    </body>
</html>

